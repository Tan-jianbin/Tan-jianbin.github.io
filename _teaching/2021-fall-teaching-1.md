---
title: "Statistical Learning"
collection: teaching
type: "Seminar"
permalink: /teaching/2021-fall-teaching-1
venue: "University of Science and Technology of China, School of Management"
date: 2021-09-01
location: "Hefei, China"

---         








































        
Prerequisites   
======

Probability, Linear Algebra, Statistical Inference, and Statistical Computation

Reference
======

1. The Elements of Statistical Learning: Data mining, Inference, and Prediction, Trevor Hastie, Robert Tibshirani, Jerome Friedman.
2. Statistical Learning of The Lasso and Generalization, Trevor Hastie, Robert Tibshirani, Martin Wainwright.
3. Statistical Foundations of Data Science, Jianqing Fan, Runze Li, Cun-Hui Zhang, Hui Zou.
4. High-Dimensional Statistics, A Non-Asymptomatic Viewpoint, Martin Wainwright.

Content 
======

1. Gaussian Model: what are statistical models, multivariate Gaussian random vector (conditional Gaussian distribution, MLE), linear discriminant analysis (MLE, QDA, LDA), sufficient dimension reduction (principal component analysis, reduced-rank LDA).
2. Statistical Learning Theory and Model Selection: decision theory (admissibility, Stein's paradox, minimax criterion), learning theory (regularization, VC dimension, minimal description length), model selection (information criterion, cross-validation).
3. Linear Regression: least square estimator (Gaussian-Markov theorem, geometry explanation, computation), subset selection (Cp criterion, forward, backward or stepwise selection), Ridge regression.
4. Optimization Methods (I): convex optimization, Lagrange multipliers, gradient decent (unconstrained gradient decent, Newton-like method), coordinate decent.
5. Linear Regression for Classification: LDA, logistic regression, SVM.
6. Generalized Linear Model: Basic introduction and algorithm, quasi-likelihood (over-dispersion), GLM with random effects.
7. EM Algorithm and Variational Inference: mixture model, GMM, mean-field method.
8. Optimization Methods (II): projected gradient descent, proximal gradient descent, ADMM.
9. Sparse Linear model: Lp shrinkage penalty, the Lasso (coordinate decent, LARS, proximal and gradient projection methods, EM, Dantzig selector, SCAD), elastic net, group Lasso, fused Lasso.
10. Undirected Graphical Model: conditional independence (clique, factorization, Hammersley-Clifford equivalence), Gaussian graphical model (circling algorithm), graphical Lasso, Gaussian copula graphical model.
11. Latent Linear Model: factor analysis, PCA, CCA, ICA.
12. Smoothing Method: basis expansions (Spline, nature cubic spline, Fourier bases, wavelet bases, smoothing spline, P-spline, GCV), kernel estimator (Kernel density estimation, NW kernel estimator, local linear regression, kernel density classifier), the curse of dimensionality.
13. Kernel Trick: kernel property (symmetric, non-negative, reproductive, reproducing kernel Hilbert space), Mercer's Theorem, kernel penalty (kernel ridge regression, kernel SVM), maximum mean discrepancy. 
14. Additive Model and Tree-Based Model: GAM, tree-based model (regression tree, classification tree), boosting (Adaboost, gradient boosting, boosting tree), bagging (random forest), ensemble learning.
15. Unsupervised Learning: clustering (K-mean, hierarchical clustering, model-based clustering, spectral clustering), sparse principal component analysis.
